{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d56e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Python library for data analysis and data frame\n",
    "import numpy as np # Numerical Python library for linear algebra and computations\n",
    "pd.set_option('display.max_columns', None) # code to display all columns\n",
    "import heapq\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "#Test split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#MODELS to test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "#Importo libres\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "#Scores\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4d1ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>-0.0269</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>2.8344</td>\n",
       "      <td>1.9480</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "ID_code                                                                      \n",
       "train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "           var_7   var_8   var_9  var_10   var_11   var_12   var_13  var_14  \\\n",
       "ID_code                                                                       \n",
       "train_0  18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745  8.7989   \n",
       "train_1  16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135  5.4345   \n",
       "train_2  14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124  7.5244   \n",
       "train_3  14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704  6.4569   \n",
       "train_4  19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895  7.7894   \n",
       "\n",
       "          var_15   var_16   var_17   var_18   var_19   var_20   var_21  \\\n",
       "ID_code                                                                  \n",
       "train_0  14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   \n",
       "train_1  13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   \n",
       "train_2  14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   \n",
       "train_3  14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   \n",
       "train_4  15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608   \n",
       "\n",
       "          var_22  var_23   var_24   var_25   var_26  var_27  var_28  var_29  \\\n",
       "ID_code                                                                       \n",
       "train_0   2.5791  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965   \n",
       "train_1   8.5524  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481   \n",
       "train_2   1.2145  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463  6.1439   \n",
       "train_3   6.8202  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070  2.8407   \n",
       "train_4  10.1102  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015  7.9352   \n",
       "\n",
       "          var_30   var_31  var_32   var_33   var_34   var_35  var_36  var_37  \\\n",
       "ID_code                                                                        \n",
       "train_0  -0.3085  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   \n",
       "train_1  -9.2987   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434   \n",
       "train_2 -11.1025  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346   \n",
       "train_3 -15.2398  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   \n",
       "train_4  -3.1582   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184   \n",
       "\n",
       "          var_38  var_39   var_40   var_41   var_42   var_43   var_44  \\\n",
       "ID_code                                                                 \n",
       "train_0   4.6667  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418   \n",
       "train_1  10.8160  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444   \n",
       "train_2  10.8917  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006   \n",
       "train_3   9.1738  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485   \n",
       "train_4  19.3038  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087   \n",
       "\n",
       "          var_45   var_46   var_47   var_48   var_49   var_50   var_51  \\\n",
       "ID_code                                                                  \n",
       "train_0  -7.0170   5.9226 -14.2136  16.0283   5.3253  12.9194  29.0460   \n",
       "train_1 -47.3797   7.3718   0.1948  34.4014  25.7037  11.8343  13.2256   \n",
       "train_2  -7.9078  11.1405  -5.7864  20.7477   6.8874  12.9143  19.5856   \n",
       "train_3 -24.6840  12.7404 -35.1659   0.7613   8.3838  12.6832   9.5503   \n",
       "train_4 -65.4863  13.8657   0.0444  -0.1346  14.4268  13.3273  10.4857   \n",
       "\n",
       "         var_52  var_53  var_54   var_55   var_56  var_57  var_58   var_59  \\\n",
       "ID_code                                                                      \n",
       "train_0 -0.6940  5.1736 -0.7474  14.8322  11.2668  5.3822  2.0183  10.1166   \n",
       "train_1 -4.1083  6.6885 -8.0946  18.5995  19.3219  7.0118  1.9210   8.8682   \n",
       "train_2  0.7268  6.4059  9.3124   6.2846  15.6372  5.8200  1.1000   9.1854   \n",
       "train_3  1.7895  5.2091  8.0913  12.3972  14.4698  6.5850  3.3164   9.4638   \n",
       "train_4 -1.4367  5.7555 -8.5414  14.1482  16.9840  6.1812  1.9548   9.2048   \n",
       "\n",
       "          var_60   var_61  var_62  var_63  var_64  var_65  var_66   var_67  \\\n",
       "ID_code                                                                      \n",
       "train_0  16.1828   4.9590  2.0771 -0.2154  8.6748  9.5319  5.8056  22.4321   \n",
       "train_1   8.0109  -7.2417  1.7944 -1.3147  8.1042  1.5365  5.4007   7.9344   \n",
       "train_2  12.5963 -10.3734  0.8748  5.8042  3.7163 -1.1016  7.3667   9.8565   \n",
       "train_3  15.7820 -25.0222  3.4418 -4.3923  8.6464  6.3072  5.6221  23.6143   \n",
       "train_4   8.6591 -27.7439 -0.4952 -1.7839  5.2670 -4.3205  6.9860   1.6184   \n",
       "\n",
       "         var_68  var_69   var_70  var_71  var_72   var_73   var_74   var_75  \\\n",
       "ID_code                                                                       \n",
       "train_0  5.0109 -4.7010  21.6374  0.5663  5.1999   8.8600  43.1127  18.3816   \n",
       "train_1  5.0220  2.2302  40.5632  0.5134  3.1701  20.1068   7.7841   7.0529   \n",
       "train_2  5.0228 -5.7828   2.3612  0.8520  6.3577  12.1719  19.7312  19.4465   \n",
       "train_3  5.0220 -3.9989   4.0462  0.2500  1.2516  24.4187   4.5290  15.4235   \n",
       "train_4  5.0301 -3.2431  40.1236  0.7737 -0.7264   4.5886  -4.5346  23.3521   \n",
       "\n",
       "          var_76   var_77  var_78   var_79   var_80   var_81   var_82  \\\n",
       "ID_code                                                                 \n",
       "train_0  -2.3440  23.4104  6.5199  12.1983  13.6468  13.8372   1.3675   \n",
       "train_1   3.2709  23.4822  5.5075  13.7814   2.5462  18.1782   0.3683   \n",
       "train_2   4.5048  23.2378  6.3191  12.8046   7.4729  15.7811  13.3529   \n",
       "train_3  11.6875  23.6273  4.0806  15.2733   0.7839  10.5404   1.6212   \n",
       "train_4   1.0273  19.1600  7.1734  14.3937   2.9598  13.3317  -9.2587   \n",
       "\n",
       "          var_83  var_84   var_85   var_86   var_87   var_88   var_89  \\\n",
       "ID_code                                                                 \n",
       "train_0   2.9423 -4.5213  21.4669   9.3225  16.4597   7.9984  -1.7069   \n",
       "train_1  -4.8210 -5.4850  13.7867 -13.5901  11.0993   7.9022  12.2301   \n",
       "train_2  10.1852  5.4604  19.0773  -4.4577   9.5413  11.9052   2.1447   \n",
       "train_3  -5.2896  1.6027  17.9762  -2.3174  15.6298   4.5474   7.5509   \n",
       "train_4  -6.7075  7.8984  14.5265   7.0799  20.1670   8.0053   3.7954   \n",
       "\n",
       "          var_90  var_91   var_92   var_93   var_94  var_95   var_96   var_97  \\\n",
       "ID_code                                                                         \n",
       "train_0 -21.4494  6.7806  11.0924   9.9913  14.8421  0.1812   8.9642  16.2572   \n",
       "train_1   0.4768  6.8852   8.0905  10.9631  11.7569 -1.2722  24.7876  26.6881   \n",
       "train_2 -22.4038  7.0883  14.1613  10.5080  14.2621  0.2647  20.4031  17.0360   \n",
       "train_3  -7.5866  7.0364  14.4027  10.7795   7.2887 -1.0930  11.3596  18.1486   \n",
       "train_4 -39.7997  7.0065   9.3627  10.4316  14.0553  0.0213  14.7246  35.2988   \n",
       "\n",
       "         var_98  var_99  var_100  var_101  var_102  var_103  var_104  var_105  \\\n",
       "ID_code                                                                         \n",
       "train_0  2.1743 -3.4132   9.4763  13.3102  26.5376   1.4403  14.7100   6.0454   \n",
       "train_1  1.8944  0.6939 -13.6950   8.4068  35.4734   1.7093  15.1866   2.6227   \n",
       "train_2  1.6981 -0.0269  -0.3939  12.6317  14.8863   1.3854  15.0284   3.9995   \n",
       "train_3  2.8344  1.9480 -19.8592  22.5316  18.6129   1.3512   9.3291   4.2835   \n",
       "train_4  1.6844  0.6715 -22.9264  12.3562  17.3410   1.6940   7.1179   5.1934   \n",
       "\n",
       "         var_106  var_107  var_108  var_109  var_110  var_111  var_112  \\\n",
       "ID_code                                                                  \n",
       "train_0   9.5426  17.1554  14.1104  24.3627   2.0323   6.7602   3.9141   \n",
       "train_1   7.3412  32.0888  13.9550  13.0858   6.6203   7.1051   5.3523   \n",
       "train_2   5.3683   8.6273  14.1963  20.3882   3.2304   5.7033   4.5255   \n",
       "train_3  10.3907   7.0874  14.3256  14.4135   4.2827   6.9750   1.6480   \n",
       "train_4   8.8230  10.6617  14.0837  28.2749  -0.1937   5.9654   1.0719   \n",
       "\n",
       "         var_113  var_114  var_115  var_116  var_117  var_118  var_119  \\\n",
       "ID_code                                                                  \n",
       "train_0  -0.4851   2.5240   1.5093   2.5516  15.5752 -13.4221   7.2739   \n",
       "train_1   8.5426   3.6159   4.1569   3.0454   7.8522 -11.5100   7.5109   \n",
       "train_2   2.1929   3.1290   2.9044   1.1696  28.7632 -17.2738   2.1056   \n",
       "train_3  11.6896   2.5762  -2.5459   5.3446  38.1015   3.5732   5.0988   \n",
       "train_4   7.9923   2.9138  -3.6135   1.4684  25.6795  13.8224   4.7478   \n",
       "\n",
       "         var_120  var_121  var_122  var_123  var_124  var_125  var_126  \\\n",
       "ID_code                                                                  \n",
       "train_0  16.0094   9.7268   0.8897   0.7754   4.2218  12.0039  13.8571   \n",
       "train_1  31.5899   9.5018   8.2736  10.1633   0.1225  12.5942  14.5697   \n",
       "train_2  21.1613   8.9573   2.7768  -2.1746   3.6932  12.4653  14.1978   \n",
       "train_3  30.5644  11.3025   3.9618  -8.2464   2.7038  12.3441  12.5431   \n",
       "train_4  41.1037  12.7140   5.2964   9.7289   3.9370  12.1316  12.5815   \n",
       "\n",
       "         var_127  var_128  var_129  var_130  var_131  var_132  var_133  \\\n",
       "ID_code                                                                  \n",
       "train_0  -0.7338  -1.9245  15.4462  12.8287   0.3587   9.6508   6.5674   \n",
       "train_1   2.4354   0.8194  16.5346  12.4205  -0.1780   5.7582   7.0513   \n",
       "train_2  -2.5511  -0.9479  17.1092  11.5419   0.0975   8.8186   6.6231   \n",
       "train_3  -1.3683   3.5974  13.9761  14.3003   1.0486   8.9500   7.1954   \n",
       "train_4   7.0642   5.6518  10.9346  11.4266   0.9442   7.7532   6.6173   \n",
       "\n",
       "         var_134  var_135  var_136  var_137  var_138  var_139  var_140  \\\n",
       "ID_code                                                                  \n",
       "train_0   5.1726   3.1345  29.4547  31.4045   2.8279  15.6599   8.3307   \n",
       "train_1   1.9568  -8.9921   9.7797  18.1577  -1.9721  16.1622   3.6937   \n",
       "train_2   3.9358 -11.7218  24.5437  15.5827   3.8212   8.6674   7.3834   \n",
       "train_3  -1.1984   1.9586  27.5609  24.6065  -2.8233   8.9821   3.8873   \n",
       "train_4  -6.8304   6.4730  17.1728  25.8128   2.6791  13.9547   6.6289   \n",
       "\n",
       "         var_141  var_142  var_143  var_144  var_145  var_146  var_147  \\\n",
       "ID_code                                                                  \n",
       "train_0  -5.6011  19.0614  11.2663   8.6989   8.3694  11.5659 -16.4727   \n",
       "train_1   6.6803  -0.3243  12.2806   8.6086  11.0738   8.9231  11.7700   \n",
       "train_2  -2.4438  10.2158   7.4844   9.1104   4.3649  11.4934   1.7624   \n",
       "train_3  15.9638  10.0142   7.8388   9.9718   2.9253  10.4994   4.1622   \n",
       "train_4  -4.3965  11.7159  16.1080   7.6874   9.1570  11.5670 -12.7047   \n",
       "\n",
       "         var_148  var_149  var_150  var_151  var_152  var_153  var_154  \\\n",
       "ID_code                                                                  \n",
       "train_0   4.0288  17.9244  18.5177  10.7800   9.0056  16.6964  10.4838   \n",
       "train_1   4.2578  -4.4223  20.6294  14.8743   9.4317  16.7242  -0.5687   \n",
       "train_2   4.0714  -1.2681  14.3330   8.0088   4.4015  14.1479  -5.1747   \n",
       "train_3   3.7613   2.3701  18.0984  17.1765   7.6508  18.2452  17.0336   \n",
       "train_4   3.7574   9.9110  20.1461   1.2995   5.8493  19.8234   4.7022   \n",
       "\n",
       "         var_155  var_156  var_157  var_158  var_159  var_160  var_161  \\\n",
       "ID_code                                                                  \n",
       "train_0   1.6573  12.1749 -13.1324  17.6054  11.5423  15.4576   5.3133   \n",
       "train_1   0.1898  12.2419  -9.6953  22.3949  10.6261  29.4846   5.8683   \n",
       "train_2   0.5778  14.5362  -1.7624  33.8820  11.6041  13.2070   5.8442   \n",
       "train_3 -10.9370  12.0500  -1.2155  19.9750  12.3892  31.8833   5.9684   \n",
       "train_4  10.6101  13.0021 -12.6068  27.0846   8.0913  33.5107   5.6953   \n",
       "\n",
       "         var_162  var_163  var_164  var_165  var_166  var_167  var_168  \\\n",
       "ID_code                                                                  \n",
       "train_0   3.6159   5.0384   6.6760  12.6644   2.7004  -0.6975   9.5981   \n",
       "train_1   3.8208  15.8348  -5.0121  15.1345   3.2003   9.3192   3.8821   \n",
       "train_2   4.7086   5.7141  -1.0410  20.5092   3.2790  -5.5952   7.3176   \n",
       "train_3   7.2084   3.8899 -11.0882  17.2502   2.5881  -2.7018   0.5641   \n",
       "train_4   5.4663  18.2201   6.5769  21.2607   3.2304  -1.7759   3.1283   \n",
       "\n",
       "         var_169  var_170  var_171  var_172  var_173  var_174  var_175  \\\n",
       "ID_code                                                                  \n",
       "train_0   5.4879  -4.7645  -8.4254  20.8773   3.1531  18.5618   7.7423   \n",
       "train_1   5.7999   5.5378   5.0988  22.0330   5.5134  30.2645  10.4968   \n",
       "train_2   5.7690  -7.0927  -3.9116   7.2569  -5.8234  25.6820  10.9202   \n",
       "train_3   5.3430  -7.1541  -6.1920  18.2366  11.7134  14.7483   8.1013   \n",
       "train_4   5.5518   1.4493  -2.6627  19.8056   2.3705  18.4685  16.3309   \n",
       "\n",
       "         var_176  var_177  var_178  var_179  var_180  var_181  var_182  \\\n",
       "ID_code                                                                  \n",
       "train_0 -10.1245  13.7241  -3.5189   1.7202  -8.4051   9.0164   3.0657   \n",
       "train_1  -7.2352  16.5721  -7.3477  11.0752  -5.5937   9.4878 -14.9100   \n",
       "train_2  -0.3104   8.8438  -9.7009   2.4013  -4.2935   9.3908 -13.2648   \n",
       "train_3  11.8771  13.9552 -10.4701   5.6961  -3.7546   8.4117   1.8986   \n",
       "train_4  -3.3456  13.5261   1.7189   5.1743  -7.6938   9.7685   4.8910   \n",
       "\n",
       "         var_183  var_184  var_185  var_186  var_187  var_188  var_189  \\\n",
       "ID_code                                                                  \n",
       "train_0  14.3691  25.8398   5.8764  11.8411 -19.7159  17.5743   0.5857   \n",
       "train_1   9.4245  22.5441  -4.8622   7.6543 -15.9319  13.3175  -0.3566   \n",
       "train_2   3.1545  23.0866  -5.3000   5.3745  -6.2660  10.1934  -0.8417   \n",
       "train_3   7.2601  -0.4639  -0.0498   7.9336 -12.8279  12.4124   1.8489   \n",
       "train_4  12.2198  11.8503  -7.8931   6.4209   5.9270  16.0201  -0.2829   \n",
       "\n",
       "         var_190  var_191  var_192  var_193  var_194  var_195  var_196  \\\n",
       "ID_code                                                                  \n",
       "train_0   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   7.8784   \n",
       "train_1   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   8.1267   \n",
       "train_2   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417  -6.5213   \n",
       "train_3   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706  -2.9275   \n",
       "train_4  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   3.9267   \n",
       "\n",
       "         var_197  var_198  var_199  \n",
       "ID_code                             \n",
       "train_0   8.5635  12.7803  -1.0914  \n",
       "train_1   8.7889  18.3560   1.9518  \n",
       "train_2   8.2675  14.7222   0.3965  \n",
       "train_3  10.2922  17.9697  -8.9996  \n",
       "train_4   9.5031  17.9974  -8.8104  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the file\n",
    "df=pd.read_csv(\"../train.csv\", sep=\",\", index_col=\"ID_code\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd1d4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target       int64\n",
       "var_0      float32\n",
       "var_1      float32\n",
       "var_2      float32\n",
       "var_3      float32\n",
       "            ...   \n",
       "var_195    float32\n",
       "var_196    float32\n",
       "var_197    float32\n",
       "var_198    float32\n",
       "var_199    float32\n",
       "Length: 201, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_float = df.select_dtypes(include=[\"float64\"]).columns.tolist() \n",
    "df[cols_float] = df[cols_float].apply(pd.to_numeric, downcast='float')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad30d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargo los datos de los dataframe\n",
    "X=df.drop(\"target\", axis=1)\n",
    "y=df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "617618ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "#Por lo anterior usamos Kfold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bfd9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#incializo PCA \n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0c7c4",
   "metadata": {},
   "source": [
    "### Genero varios PCA para distintas cantidades de dimensiones para luego calcular el accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d3b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 1 variables\n",
    "pca_1=PCA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4c5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 3 variables\n",
    "pca_3=PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1112d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 10 variables\n",
    "pca_10=PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6d5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 30 variables\n",
    "pca_30=PCA(n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaccf70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 100 variables\n",
    "pca_100=PCA(n_components=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc687b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_pca=[pca_1,pca_3,pca_10,pca_30,pca_100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5bc5c",
   "metadata": {},
   "source": [
    "## INICIALIZO TODOS LOS MODELOS PARA PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f216065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializo modelos LogisticRegression para primer test\n",
    "lg=LogisticRegression(random_state=0,max_iter=5000)\n",
    "lg_30=LogisticRegression(random_state=0,max_iter=5000)\n",
    "lg_100=LogisticRegression(random_state=0,max_iter=5000)\n",
    "\n",
    "#Inicializo modelos Randomforest para primer test\n",
    "model_rf = RandomForestClassifier(max_depth=15, random_state=0)\n",
    "model_rf_30=RandomForestClassifier(max_depth=15, random_state=0)\n",
    "model_rf_100=RandomForestClassifier(max_depth=15, random_state=0)\n",
    "\n",
    "#Inicializo Xboost para primer test\n",
    "xgboost=XGBClassifier()\n",
    "xgboost_30=XGBClassifier()\n",
    "xgboost_100=XGBClassifier()\n",
    "\n",
    "#Inicializo Catboost para primer test\n",
    "catboost=CatBoostClassifier(learning_rate=0.03, iterations=15)\n",
    "catboost_100=CatBoostClassifier(learning_rate=0.03, iterations=15)\n",
    "catboost_100=CatBoostClassifier(learning_rate=0.03, iterations=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47105918",
   "metadata": {},
   "source": [
    "## Comportamiento de LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7fe1cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35446   535]\n",
      " [ 2956  1063]] 0.6652065081351689 0.26449365513809403 0.37849385793128004\n",
      "[[35510   471]\n",
      " [ 2928  1091]] 0.6984635083226632 0.27146056232893756 0.39096936032969\n"
     ]
    }
   ],
   "source": [
    "contador=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X.iloc[train_index]\n",
    "    X_test=X.iloc[test_index]\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    lg.fit(X_train,y_train)\n",
    "    y_pred=np.asarray(lg.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')    \n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e23aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35981     0]\n",
      " [ 4018     1]] 1.0 0.0002488181139586962 0.0004975124378109454\n",
      "[[35981     0]\n",
      " [ 4017     2]] 1.0 0.0004976362279173924 0.000994777418552599\n"
     ]
    }
   ],
   "source": [
    "contador=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_30.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_30.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    lg_30.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(lg_30.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "654778ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17991     0]\n",
      " [ 2009     0]] 0.89955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m y_train_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[train_index]\n\u001b[0;32m      5\u001b[0m y_test_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel_rf_100\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y_pred_1\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39masarray(model_rf_100\u001b[38;5;241m.\u001b[39mpredict(X_test_1))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#precision = precision_score(y_test_1, y_pred_1,average='micro')\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train_1=pca_100.fit_transform(X.iloc[train_index])\n",
    "    X_test_1=pca_100.transform(X.iloc[test_index])\n",
    "    y_train_1=y.iloc[train_index]\n",
    "    y_test_1=y.iloc[test_index]\n",
    "    model_rf_100.fit(X_train_1, y_train_1)\n",
    "    y_pred_1=np.asarray(model_rf_100.predict(X_test_1))\n",
    "    #precision = precision_score(y_test_1, y_pred_1,average='micro')\n",
    "    cm=confusion_matrix(y_test_1, y_pred_1)\n",
    "    recall = recall_score(y_test_1, y_pred_1,average='micro')\n",
    "    #f1 = f1_score(y_test_1, y_pred_1)\n",
    "    print(cm,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b4c9bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17991     0]\n",
      " [ 2009     0]] 0.89955\n",
      "[[17991     0]\n",
      " [ 2009     0]] 0.89955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m y_train_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[train_index]\n\u001b[0;32m      5\u001b[0m y_test_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y_pred_1\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39masarray(model_rf\u001b[38;5;241m.\u001b[39mpredict(X_test_1))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#precision = precision_score(y_test_1, y_pred_1,average='micro')\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train_1=X.iloc[train_index]\n",
    "    X_test_1=X.iloc[test_index]\n",
    "    y_train_1=y.iloc[train_index]\n",
    "    y_test_1=y.iloc[test_index]\n",
    "    model_rf.fit(X_train_1, y_train_1)\n",
    "    y_pred_1=np.asarray(model_rf.predict(X_test_1))\n",
    "    #precision = precision_score(y_test_1, y_pred_1,average='micro')\n",
    "    cm=confusion_matrix(y_test_1, y_pred_1)\n",
    "    recall = recall_score(y_test_1, y_pred_1,average='micro')\n",
    "    #f1 = f1_score(y_test_1, y_pred_1)\n",
    "    print(cm,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "021254a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17951    40]\n",
      " [ 1990    19]] 0.8985\n",
      "[[17954    37]\n",
      " [ 1989    20]] 0.8987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m y_train_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[train_index]\n\u001b[0;32m      5\u001b[0m y_test_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mXGBClassifier_30\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y_pred_1\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39masarray(XGBClassifier_30\u001b[38;5;241m.\u001b[39mpredict(X_test_1))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#precision = precision_score(y_test_1, y_pred_1,average='micro')\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1381\u001b[0m )\n\u001b[0;32m   1382\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1383\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1384\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1397\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_categorical,\n\u001b[0;32m   1398\u001b[0m )\n\u001b[1;32m-> 1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:1778\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train_1=pca_30.fit_transform(X.iloc[train_index])\n",
    "    X_test_1=pca_30.transform(X.iloc[test_index])\n",
    "    y_train_1=y.iloc[train_index]\n",
    "    y_test_1=y.iloc[test_index]\n",
    "    XGBClassifier_30.fit(X_train_1, y_train_1)\n",
    "    y_pred_1=np.asarray(XGBClassifier_30.predict(X_test_1))\n",
    "    precision = precision_score(y_test_1, y_pred_1,average='micro')\n",
    "    cm=confusion_matrix(y_test_1, y_pred_1)\n",
    "    recall = recall_score(y_test_1, y_pred_1,average='micro')\n",
    "    f1 = f1_score(y_test_1, y_pred_1,average='micro')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc9a968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17794   197]\n",
      " [ 1758   251]] 0.5602678571428571 0.1249377799900448\n",
      "[[17766   225]\n",
      " [ 1764   245]] 0.5212765957446809 0.12195121951219512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m y_train_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[train_index]\n\u001b[0;32m      5\u001b[0m y_test_1\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mXGBClassifier_100\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y_pred_1\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39masarray(XGBClassifier_100\u001b[38;5;241m.\u001b[39mpredict(X_test_1))\n\u001b[0;32m      8\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test_1, y_pred_1,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1381\u001b[0m )\n\u001b[0;32m   1382\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1383\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1384\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1397\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_categorical,\n\u001b[0;32m   1398\u001b[0m )\n\u001b[1;32m-> 1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:1778\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train_1=pca_100.fit_transform(X.iloc[train_index])\n",
    "    X_test_1=pca_100.transform(X.iloc[test_index])\n",
    "    y_train_1=y.iloc[train_index]\n",
    "    y_test_1=y.iloc[test_index]\n",
    "    XGBClassifier_100.fit(X_train_1, y_train_1)\n",
    "    y_pred_1=np.asarray(XGBClassifier_100.predict(X_test_1))\n",
    "    precision = precision_score(y_test_1, y_pred_1,average='micro') #Hago binary proque me interesan los positivos del 1 \n",
    "    cm=confusion_matrix(y_test_1, y_pred_1)\n",
    "    recall = recall_score(y_test_1, y_pred_1,average='micro') #Hago binary proque me interesan los positivos del 1 \n",
    "    f1 = f1_score(y_test_1, y_pred_1,average='micro') #Hago binary proque me interesan los positivos del 1 \n",
    "    print(cm,precision,recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
