{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d56e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Python library for data analysis and data frame\n",
    "import numpy as np # Numerical Python library for linear algebra and computations\n",
    "pd.set_option('display.max_columns', None) # code to display all columns\n",
    "import heapq\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "#Test split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#MODELS to test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "#Importo libres\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "#Scores\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4d1ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>-0.0269</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>2.8344</td>\n",
       "      <td>1.9480</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "ID_code                                                                      \n",
       "train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "           var_7   var_8   var_9  var_10   var_11   var_12   var_13  var_14  \\\n",
       "ID_code                                                                       \n",
       "train_0  18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745  8.7989   \n",
       "train_1  16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135  5.4345   \n",
       "train_2  14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124  7.5244   \n",
       "train_3  14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704  6.4569   \n",
       "train_4  19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895  7.7894   \n",
       "\n",
       "          var_15   var_16   var_17   var_18   var_19   var_20   var_21  \\\n",
       "ID_code                                                                  \n",
       "train_0  14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   \n",
       "train_1  13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   \n",
       "train_2  14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   \n",
       "train_3  14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   \n",
       "train_4  15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608   \n",
       "\n",
       "          var_22  var_23   var_24   var_25   var_26  var_27  var_28  var_29  \\\n",
       "ID_code                                                                       \n",
       "train_0   2.5791  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965   \n",
       "train_1   8.5524  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481   \n",
       "train_2   1.2145  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463  6.1439   \n",
       "train_3   6.8202  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070  2.8407   \n",
       "train_4  10.1102  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015  7.9352   \n",
       "\n",
       "          var_30   var_31  var_32   var_33   var_34   var_35  var_36  var_37  \\\n",
       "ID_code                                                                        \n",
       "train_0  -0.3085  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   \n",
       "train_1  -9.2987   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434   \n",
       "train_2 -11.1025  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346   \n",
       "train_3 -15.2398  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   \n",
       "train_4  -3.1582   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184   \n",
       "\n",
       "          var_38  var_39   var_40   var_41   var_42   var_43   var_44  \\\n",
       "ID_code                                                                 \n",
       "train_0   4.6667  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418   \n",
       "train_1  10.8160  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444   \n",
       "train_2  10.8917  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006   \n",
       "train_3   9.1738  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485   \n",
       "train_4  19.3038  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087   \n",
       "\n",
       "          var_45   var_46   var_47   var_48   var_49   var_50   var_51  \\\n",
       "ID_code                                                                  \n",
       "train_0  -7.0170   5.9226 -14.2136  16.0283   5.3253  12.9194  29.0460   \n",
       "train_1 -47.3797   7.3718   0.1948  34.4014  25.7037  11.8343  13.2256   \n",
       "train_2  -7.9078  11.1405  -5.7864  20.7477   6.8874  12.9143  19.5856   \n",
       "train_3 -24.6840  12.7404 -35.1659   0.7613   8.3838  12.6832   9.5503   \n",
       "train_4 -65.4863  13.8657   0.0444  -0.1346  14.4268  13.3273  10.4857   \n",
       "\n",
       "         var_52  var_53  var_54   var_55   var_56  var_57  var_58   var_59  \\\n",
       "ID_code                                                                      \n",
       "train_0 -0.6940  5.1736 -0.7474  14.8322  11.2668  5.3822  2.0183  10.1166   \n",
       "train_1 -4.1083  6.6885 -8.0946  18.5995  19.3219  7.0118  1.9210   8.8682   \n",
       "train_2  0.7268  6.4059  9.3124   6.2846  15.6372  5.8200  1.1000   9.1854   \n",
       "train_3  1.7895  5.2091  8.0913  12.3972  14.4698  6.5850  3.3164   9.4638   \n",
       "train_4 -1.4367  5.7555 -8.5414  14.1482  16.9840  6.1812  1.9548   9.2048   \n",
       "\n",
       "          var_60   var_61  var_62  var_63  var_64  var_65  var_66   var_67  \\\n",
       "ID_code                                                                      \n",
       "train_0  16.1828   4.9590  2.0771 -0.2154  8.6748  9.5319  5.8056  22.4321   \n",
       "train_1   8.0109  -7.2417  1.7944 -1.3147  8.1042  1.5365  5.4007   7.9344   \n",
       "train_2  12.5963 -10.3734  0.8748  5.8042  3.7163 -1.1016  7.3667   9.8565   \n",
       "train_3  15.7820 -25.0222  3.4418 -4.3923  8.6464  6.3072  5.6221  23.6143   \n",
       "train_4   8.6591 -27.7439 -0.4952 -1.7839  5.2670 -4.3205  6.9860   1.6184   \n",
       "\n",
       "         var_68  var_69   var_70  var_71  var_72   var_73   var_74   var_75  \\\n",
       "ID_code                                                                       \n",
       "train_0  5.0109 -4.7010  21.6374  0.5663  5.1999   8.8600  43.1127  18.3816   \n",
       "train_1  5.0220  2.2302  40.5632  0.5134  3.1701  20.1068   7.7841   7.0529   \n",
       "train_2  5.0228 -5.7828   2.3612  0.8520  6.3577  12.1719  19.7312  19.4465   \n",
       "train_3  5.0220 -3.9989   4.0462  0.2500  1.2516  24.4187   4.5290  15.4235   \n",
       "train_4  5.0301 -3.2431  40.1236  0.7737 -0.7264   4.5886  -4.5346  23.3521   \n",
       "\n",
       "          var_76   var_77  var_78   var_79   var_80   var_81   var_82  \\\n",
       "ID_code                                                                 \n",
       "train_0  -2.3440  23.4104  6.5199  12.1983  13.6468  13.8372   1.3675   \n",
       "train_1   3.2709  23.4822  5.5075  13.7814   2.5462  18.1782   0.3683   \n",
       "train_2   4.5048  23.2378  6.3191  12.8046   7.4729  15.7811  13.3529   \n",
       "train_3  11.6875  23.6273  4.0806  15.2733   0.7839  10.5404   1.6212   \n",
       "train_4   1.0273  19.1600  7.1734  14.3937   2.9598  13.3317  -9.2587   \n",
       "\n",
       "          var_83  var_84   var_85   var_86   var_87   var_88   var_89  \\\n",
       "ID_code                                                                 \n",
       "train_0   2.9423 -4.5213  21.4669   9.3225  16.4597   7.9984  -1.7069   \n",
       "train_1  -4.8210 -5.4850  13.7867 -13.5901  11.0993   7.9022  12.2301   \n",
       "train_2  10.1852  5.4604  19.0773  -4.4577   9.5413  11.9052   2.1447   \n",
       "train_3  -5.2896  1.6027  17.9762  -2.3174  15.6298   4.5474   7.5509   \n",
       "train_4  -6.7075  7.8984  14.5265   7.0799  20.1670   8.0053   3.7954   \n",
       "\n",
       "          var_90  var_91   var_92   var_93   var_94  var_95   var_96   var_97  \\\n",
       "ID_code                                                                         \n",
       "train_0 -21.4494  6.7806  11.0924   9.9913  14.8421  0.1812   8.9642  16.2572   \n",
       "train_1   0.4768  6.8852   8.0905  10.9631  11.7569 -1.2722  24.7876  26.6881   \n",
       "train_2 -22.4038  7.0883  14.1613  10.5080  14.2621  0.2647  20.4031  17.0360   \n",
       "train_3  -7.5866  7.0364  14.4027  10.7795   7.2887 -1.0930  11.3596  18.1486   \n",
       "train_4 -39.7997  7.0065   9.3627  10.4316  14.0553  0.0213  14.7246  35.2988   \n",
       "\n",
       "         var_98  var_99  var_100  var_101  var_102  var_103  var_104  var_105  \\\n",
       "ID_code                                                                         \n",
       "train_0  2.1743 -3.4132   9.4763  13.3102  26.5376   1.4403  14.7100   6.0454   \n",
       "train_1  1.8944  0.6939 -13.6950   8.4068  35.4734   1.7093  15.1866   2.6227   \n",
       "train_2  1.6981 -0.0269  -0.3939  12.6317  14.8863   1.3854  15.0284   3.9995   \n",
       "train_3  2.8344  1.9480 -19.8592  22.5316  18.6129   1.3512   9.3291   4.2835   \n",
       "train_4  1.6844  0.6715 -22.9264  12.3562  17.3410   1.6940   7.1179   5.1934   \n",
       "\n",
       "         var_106  var_107  var_108  var_109  var_110  var_111  var_112  \\\n",
       "ID_code                                                                  \n",
       "train_0   9.5426  17.1554  14.1104  24.3627   2.0323   6.7602   3.9141   \n",
       "train_1   7.3412  32.0888  13.9550  13.0858   6.6203   7.1051   5.3523   \n",
       "train_2   5.3683   8.6273  14.1963  20.3882   3.2304   5.7033   4.5255   \n",
       "train_3  10.3907   7.0874  14.3256  14.4135   4.2827   6.9750   1.6480   \n",
       "train_4   8.8230  10.6617  14.0837  28.2749  -0.1937   5.9654   1.0719   \n",
       "\n",
       "         var_113  var_114  var_115  var_116  var_117  var_118  var_119  \\\n",
       "ID_code                                                                  \n",
       "train_0  -0.4851   2.5240   1.5093   2.5516  15.5752 -13.4221   7.2739   \n",
       "train_1   8.5426   3.6159   4.1569   3.0454   7.8522 -11.5100   7.5109   \n",
       "train_2   2.1929   3.1290   2.9044   1.1696  28.7632 -17.2738   2.1056   \n",
       "train_3  11.6896   2.5762  -2.5459   5.3446  38.1015   3.5732   5.0988   \n",
       "train_4   7.9923   2.9138  -3.6135   1.4684  25.6795  13.8224   4.7478   \n",
       "\n",
       "         var_120  var_121  var_122  var_123  var_124  var_125  var_126  \\\n",
       "ID_code                                                                  \n",
       "train_0  16.0094   9.7268   0.8897   0.7754   4.2218  12.0039  13.8571   \n",
       "train_1  31.5899   9.5018   8.2736  10.1633   0.1225  12.5942  14.5697   \n",
       "train_2  21.1613   8.9573   2.7768  -2.1746   3.6932  12.4653  14.1978   \n",
       "train_3  30.5644  11.3025   3.9618  -8.2464   2.7038  12.3441  12.5431   \n",
       "train_4  41.1037  12.7140   5.2964   9.7289   3.9370  12.1316  12.5815   \n",
       "\n",
       "         var_127  var_128  var_129  var_130  var_131  var_132  var_133  \\\n",
       "ID_code                                                                  \n",
       "train_0  -0.7338  -1.9245  15.4462  12.8287   0.3587   9.6508   6.5674   \n",
       "train_1   2.4354   0.8194  16.5346  12.4205  -0.1780   5.7582   7.0513   \n",
       "train_2  -2.5511  -0.9479  17.1092  11.5419   0.0975   8.8186   6.6231   \n",
       "train_3  -1.3683   3.5974  13.9761  14.3003   1.0486   8.9500   7.1954   \n",
       "train_4   7.0642   5.6518  10.9346  11.4266   0.9442   7.7532   6.6173   \n",
       "\n",
       "         var_134  var_135  var_136  var_137  var_138  var_139  var_140  \\\n",
       "ID_code                                                                  \n",
       "train_0   5.1726   3.1345  29.4547  31.4045   2.8279  15.6599   8.3307   \n",
       "train_1   1.9568  -8.9921   9.7797  18.1577  -1.9721  16.1622   3.6937   \n",
       "train_2   3.9358 -11.7218  24.5437  15.5827   3.8212   8.6674   7.3834   \n",
       "train_3  -1.1984   1.9586  27.5609  24.6065  -2.8233   8.9821   3.8873   \n",
       "train_4  -6.8304   6.4730  17.1728  25.8128   2.6791  13.9547   6.6289   \n",
       "\n",
       "         var_141  var_142  var_143  var_144  var_145  var_146  var_147  \\\n",
       "ID_code                                                                  \n",
       "train_0  -5.6011  19.0614  11.2663   8.6989   8.3694  11.5659 -16.4727   \n",
       "train_1   6.6803  -0.3243  12.2806   8.6086  11.0738   8.9231  11.7700   \n",
       "train_2  -2.4438  10.2158   7.4844   9.1104   4.3649  11.4934   1.7624   \n",
       "train_3  15.9638  10.0142   7.8388   9.9718   2.9253  10.4994   4.1622   \n",
       "train_4  -4.3965  11.7159  16.1080   7.6874   9.1570  11.5670 -12.7047   \n",
       "\n",
       "         var_148  var_149  var_150  var_151  var_152  var_153  var_154  \\\n",
       "ID_code                                                                  \n",
       "train_0   4.0288  17.9244  18.5177  10.7800   9.0056  16.6964  10.4838   \n",
       "train_1   4.2578  -4.4223  20.6294  14.8743   9.4317  16.7242  -0.5687   \n",
       "train_2   4.0714  -1.2681  14.3330   8.0088   4.4015  14.1479  -5.1747   \n",
       "train_3   3.7613   2.3701  18.0984  17.1765   7.6508  18.2452  17.0336   \n",
       "train_4   3.7574   9.9110  20.1461   1.2995   5.8493  19.8234   4.7022   \n",
       "\n",
       "         var_155  var_156  var_157  var_158  var_159  var_160  var_161  \\\n",
       "ID_code                                                                  \n",
       "train_0   1.6573  12.1749 -13.1324  17.6054  11.5423  15.4576   5.3133   \n",
       "train_1   0.1898  12.2419  -9.6953  22.3949  10.6261  29.4846   5.8683   \n",
       "train_2   0.5778  14.5362  -1.7624  33.8820  11.6041  13.2070   5.8442   \n",
       "train_3 -10.9370  12.0500  -1.2155  19.9750  12.3892  31.8833   5.9684   \n",
       "train_4  10.6101  13.0021 -12.6068  27.0846   8.0913  33.5107   5.6953   \n",
       "\n",
       "         var_162  var_163  var_164  var_165  var_166  var_167  var_168  \\\n",
       "ID_code                                                                  \n",
       "train_0   3.6159   5.0384   6.6760  12.6644   2.7004  -0.6975   9.5981   \n",
       "train_1   3.8208  15.8348  -5.0121  15.1345   3.2003   9.3192   3.8821   \n",
       "train_2   4.7086   5.7141  -1.0410  20.5092   3.2790  -5.5952   7.3176   \n",
       "train_3   7.2084   3.8899 -11.0882  17.2502   2.5881  -2.7018   0.5641   \n",
       "train_4   5.4663  18.2201   6.5769  21.2607   3.2304  -1.7759   3.1283   \n",
       "\n",
       "         var_169  var_170  var_171  var_172  var_173  var_174  var_175  \\\n",
       "ID_code                                                                  \n",
       "train_0   5.4879  -4.7645  -8.4254  20.8773   3.1531  18.5618   7.7423   \n",
       "train_1   5.7999   5.5378   5.0988  22.0330   5.5134  30.2645  10.4968   \n",
       "train_2   5.7690  -7.0927  -3.9116   7.2569  -5.8234  25.6820  10.9202   \n",
       "train_3   5.3430  -7.1541  -6.1920  18.2366  11.7134  14.7483   8.1013   \n",
       "train_4   5.5518   1.4493  -2.6627  19.8056   2.3705  18.4685  16.3309   \n",
       "\n",
       "         var_176  var_177  var_178  var_179  var_180  var_181  var_182  \\\n",
       "ID_code                                                                  \n",
       "train_0 -10.1245  13.7241  -3.5189   1.7202  -8.4051   9.0164   3.0657   \n",
       "train_1  -7.2352  16.5721  -7.3477  11.0752  -5.5937   9.4878 -14.9100   \n",
       "train_2  -0.3104   8.8438  -9.7009   2.4013  -4.2935   9.3908 -13.2648   \n",
       "train_3  11.8771  13.9552 -10.4701   5.6961  -3.7546   8.4117   1.8986   \n",
       "train_4  -3.3456  13.5261   1.7189   5.1743  -7.6938   9.7685   4.8910   \n",
       "\n",
       "         var_183  var_184  var_185  var_186  var_187  var_188  var_189  \\\n",
       "ID_code                                                                  \n",
       "train_0  14.3691  25.8398   5.8764  11.8411 -19.7159  17.5743   0.5857   \n",
       "train_1   9.4245  22.5441  -4.8622   7.6543 -15.9319  13.3175  -0.3566   \n",
       "train_2   3.1545  23.0866  -5.3000   5.3745  -6.2660  10.1934  -0.8417   \n",
       "train_3   7.2601  -0.4639  -0.0498   7.9336 -12.8279  12.4124   1.8489   \n",
       "train_4  12.2198  11.8503  -7.8931   6.4209   5.9270  16.0201  -0.2829   \n",
       "\n",
       "         var_190  var_191  var_192  var_193  var_194  var_195  var_196  \\\n",
       "ID_code                                                                  \n",
       "train_0   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   7.8784   \n",
       "train_1   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   8.1267   \n",
       "train_2   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417  -6.5213   \n",
       "train_3   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706  -2.9275   \n",
       "train_4  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   3.9267   \n",
       "\n",
       "         var_197  var_198  var_199  \n",
       "ID_code                             \n",
       "train_0   8.5635  12.7803  -1.0914  \n",
       "train_1   8.7889  18.3560   1.9518  \n",
       "train_2   8.2675  14.7222   0.3965  \n",
       "train_3  10.2922  17.9697  -8.9996  \n",
       "train_4   9.5031  17.9974  -8.8104  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the file\n",
    "df=pd.read_csv(\"../train.csv\", sep=\",\", index_col=\"ID_code\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd1d4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target       int64\n",
       "var_0      float32\n",
       "var_1      float32\n",
       "var_2      float32\n",
       "var_3      float32\n",
       "            ...   \n",
       "var_195    float32\n",
       "var_196    float32\n",
       "var_197    float32\n",
       "var_198    float32\n",
       "var_199    float32\n",
       "Length: 201, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_float = df.select_dtypes(include=[\"float64\"]).columns.tolist() \n",
    "df[cols_float] = df[cols_float].apply(pd.to_numeric, downcast='float')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad30d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargo los datos de los dataframe\n",
    "X=df.drop(\"target\", axis=1)\n",
    "y=df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "617618ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "#Por lo anterior usamos Kfold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bfd9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#incializo PCA \n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0c7c4",
   "metadata": {},
   "source": [
    "### Genero varios PCA para distintas cantidades de dimensiones para luego calcular el accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d3b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 1 variables\n",
    "pca_1=PCA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4c5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 3 variables\n",
    "pca_3=PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1112d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 10 variables\n",
    "pca_10=PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6d5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 30 variables\n",
    "pca_30=PCA(n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaccf70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 100 variables\n",
    "pca_100=PCA(n_components=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc687b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_pca=[pca_1,pca_3,pca_10,pca_30,pca_100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5bc5c",
   "metadata": {},
   "source": [
    "## PRUBEAS POR DISTINTOS MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab417d5",
   "metadata": {},
   "source": [
    "## Comportamiento de LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55b3856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializo modelos LogisticRegression para primer test\n",
    "lg=LogisticRegression(random_state=0,max_iter=5000)\n",
    "lg_30=LogisticRegression(random_state=0,max_iter=5000)\n",
    "lg_100=LogisticRegression(random_state=0,max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "577905a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35446   535]\n",
      " [ 2956  1063]] 0.6652065081351689 0.26449365513809403 0.37849385793128004\n",
      "[[35510   471]\n",
      " [ 2928  1091]] 0.6984635083226632 0.27146056232893756 0.39096936032969\n"
     ]
    }
   ],
   "source": [
    "#==================================LogisticRegression================================================\n",
    "results=[]\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X.iloc[train_index]\n",
    "    X_test=X.iloc[test_index]\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    lg.fit(X_train,y_train)\n",
    "    y_pred=np.asarray(lg.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')    \n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    print(cm,precision,recall,f1)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8e23aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n",
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#==================================LogisticRegression================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_30.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_30.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    lg_30.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(lg_30.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "654778ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35788   193]\n",
      " [ 3658   361]] 0.6516245487364621 0.08982333913908933 0.15788322764049859\n",
      "[[35802   179]\n",
      " [ 3689   330]] 0.6483300589390962 0.08210997760636975 0.14575971731448764\n"
     ]
    }
   ],
   "source": [
    "#==================================LogisticRegression================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_100.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_100.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    lg_100.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(lg_100.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a1381",
   "metadata": {},
   "source": [
    "## Comportamiento de Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39f60c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializo modelos Randomforest para primer test\n",
    "model_rf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "model_rf_30=RandomForestClassifier(max_depth=20, random_state=0)\n",
    "model_rf_100=RandomForestClassifier(max_depth=20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b4c9bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n",
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#==================================Randomforest================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X.iloc[train_index]\n",
    "    X_test=X.iloc[test_index]\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    model_rf.fit(X_train,y_train)\n",
    "    y_pred=np.asarray(model_rf.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')    \n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "718c865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n",
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#==================================Randomforest================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_30.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_30.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    model_rf_30.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(model_rf_30.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c9c5a3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n",
      "[[35981     0]\n",
      " [ 4019     0]] 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#==================================Randomforest================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_100.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_100.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    model_rf_100.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(model_rf_100.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e599d",
   "metadata": {},
   "source": [
    "## Comportamiento de XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b82bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializo Xboost para primer test\n",
    "xgboost=XGBClassifier()\n",
    "xgboost_30=XGBClassifier()\n",
    "xgboost_100=XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "108a59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35549   432]\n",
      " [ 3032   987]] 0.6955602536997886 0.24558347847723314 0.3630011033468187\n",
      "[[35573   408]\n",
      " [ 3068   951]] 0.6997792494481236 0.23662602637472008 0.35366307177389367\n"
     ]
    }
   ],
   "source": [
    "#==================================XGBClassifier================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X.iloc[train_index]\n",
    "    X_test=X.iloc[test_index]\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    xgboost.fit(X_train,y_train)\n",
    "    y_pred=np.asarray(xgboost.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')    \n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9959a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35885    96]\n",
      " [ 3983    36]] 0.2727272727272727 0.008957452102513063 0.017345218019754274\n",
      "[[35909    72]\n",
      " [ 3983    36]] 0.3333333333333333 0.008957452102513063 0.017446086745820207\n"
     ]
    }
   ],
   "source": [
    "#==================================XGBClassifier================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_30.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_30.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    xgboost_30.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(xgboost_30.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "745fe28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35550   431]\n",
      " [ 3547   472]] 0.5227021040974529 0.1174421497885046 0.19179195449004469\n",
      "[[35585   396]\n",
      " [ 3530   489]] 0.5525423728813559 0.12167205772580243 0.1994290375203915\n"
     ]
    }
   ],
   "source": [
    "#==================================XGBClassifier================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_100.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_100.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    xgboost_100.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(xgboost_100.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7d220",
   "metadata": {},
   "source": [
    "## Comportamiento de Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ea89bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializo Catboost para primer test\n",
    "catboost=CatBoostClassifier(learning_rate=0.2, iterations=50, max_depth= 10,logging_level=\"Silent\")\n",
    "catboost_30=CatBoostClassifier(learning_rate=0.2, iterations=50, max_depth= 10,logging_level=\"Silent\")\n",
    "catboost_100=CatBoostClassifier(learning_rate=0.2, iterations=50, max_depth= 10,logging_level=\"Silent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a85af40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35914    67]\n",
      " [ 3627   392]] 0.8540305010893247 0.0975367006718089 0.17507815989280928\n",
      "[[35903    78]\n",
      " [ 3631   388]] 0.8326180257510729 0.09654142821597413 0.1730211817168339\n"
     ]
    }
   ],
   "source": [
    "#==================================Catboost================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X.iloc[train_index]\n",
    "    X_test=X.iloc[test_index]\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    catboost.fit(X_train,y_train)\n",
    "    y_pred=np.asarray(catboost.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')    \n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "757bd74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35970    11]\n",
      " [ 4008    11]] 0.5 0.002736999253545658 0.00544419698094531\n",
      "[[35964    17]\n",
      " [ 4004    15]] 0.46875 0.003732271709380443 0.007405578869414959\n"
     ]
    }
   ],
   "source": [
    "#==================================Catboost================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_30.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_30.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    catboost_30.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(catboost_30.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d3532386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35933    48]\n",
      " [ 3905   114]] 0.7037037037037037 0.028365264991291366 0.054532408514709405\n",
      "[[35943    38]\n",
      " [ 3897   122]] 0.7625 0.030355809902960936 0.05838717396506341\n"
     ]
    }
   ],
   "source": [
    "#==================================Catboost================================================\n",
    "contador=0\n",
    "lista=[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=pca_100.fit_transform(X.iloc[train_index])\n",
    "    X_test=pca_100.transform(X.iloc[test_index])\n",
    "    y_train=y.iloc[train_index]\n",
    "    y_test=y.iloc[test_index]\n",
    "    catboost_100.fit(X_train, y_train)\n",
    "    y_pred=np.asarray(catboost_100.predict(X_test))\n",
    "    cm=confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='binary')\n",
    "    recall = recall_score(y_test, y_pred,average='binary')\n",
    "    f1 = f1_score(y_test, y_pred,average='binary')\n",
    "    print(cm,precision,recall,f1)\n",
    "    lista=[precision,recall]\n",
    "    results.append(lista)\n",
    "    contador+=1\n",
    "    if contador==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "09cb9dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filas=[\"LogisticRegression_fulldata\",\"LogisticRegression_fulldata\",\n",
    "       \"LogisticRegression_pca30\",\"LogisticRegression_pca30\",\n",
    "       \"LogisticRegression_pca100\",\"LogisticRegression_pca100\",\n",
    "       \"Randomforest_fulldata\", \"Randomforest_fulldata\",\n",
    "       \"Randomforest_pca30\",\"Randomforest_pca30\",\n",
    "       \"Randomforest_pca100\", \"Randomforest_pca100\",\n",
    "      \"RXGBClassifier_fulldata\",\"RXGBClassifier_fulldata\",\n",
    "       \"RXGBClassifier_pca30\",\"RXGBClassifier_pca30\",\n",
    "       \"XGBClassifier_pca100\",\"XGBClassifier_pca100\",\n",
    "      \"Catboost_fulldata\",\"Catboost_fulldata\",\n",
    "       \"Catboost_pca30\",\"Catboost_pca30\",\n",
    "       \"Catboost_pca100\",\"Catboost_pca100\"]\n",
    "columnas=[\"PRECISION\", \"RECALL\"]\n",
    "len(filas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "572c6886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>RECALL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_fulldata</th>\n",
       "      <td>0.698464</td>\n",
       "      <td>0.271461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_fulldata</th>\n",
       "      <td>0.665207</td>\n",
       "      <td>0.264494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RXGBClassifier_fulldata</th>\n",
       "      <td>0.695560</td>\n",
       "      <td>0.245583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RXGBClassifier_fulldata</th>\n",
       "      <td>0.699779</td>\n",
       "      <td>0.236626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier_pca100</th>\n",
       "      <td>0.552542</td>\n",
       "      <td>0.121672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier_pca100</th>\n",
       "      <td>0.522702</td>\n",
       "      <td>0.117442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catboost_fulldata</th>\n",
       "      <td>0.854031</td>\n",
       "      <td>0.097537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catboost_fulldata</th>\n",
       "      <td>0.832618</td>\n",
       "      <td>0.096541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_pca100</th>\n",
       "      <td>0.651625</td>\n",
       "      <td>0.089823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_pca100</th>\n",
       "      <td>0.648330</td>\n",
       "      <td>0.082110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catboost_pca100</th>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.030356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catboost_pca100</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.028365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RXGBClassifier_pca30</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.008957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RXGBClassifier_pca30</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.008957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catboost_pca30</th>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.003732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catboost_pca30</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_pca30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_pca30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randomforest_fulldata</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randomforest_fulldata</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randomforest_pca30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randomforest_pca30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randomforest_pca100</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randomforest_pca100</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             PRECISION    RECALL\n",
       "LogisticRegression_fulldata   0.698464  0.271461\n",
       "LogisticRegression_fulldata   0.665207  0.264494\n",
       "RXGBClassifier_fulldata       0.695560  0.245583\n",
       "RXGBClassifier_fulldata       0.699779  0.236626\n",
       "XGBClassifier_pca100          0.552542  0.121672\n",
       "XGBClassifier_pca100          0.522702  0.117442\n",
       "Catboost_fulldata             0.854031  0.097537\n",
       "Catboost_fulldata             0.832618  0.096541\n",
       "LogisticRegression_pca100     0.651625  0.089823\n",
       "LogisticRegression_pca100     0.648330  0.082110\n",
       "Catboost_pca100               0.762500  0.030356\n",
       "Catboost_pca100               0.703704  0.028365\n",
       "RXGBClassifier_pca30          0.333333  0.008957\n",
       "RXGBClassifier_pca30          0.272727  0.008957\n",
       "Catboost_pca30                0.468750  0.003732\n",
       "Catboost_pca30                0.500000  0.002737\n",
       "LogisticRegression_pca30      0.000000  0.000000\n",
       "LogisticRegression_pca30      0.000000  0.000000\n",
       "Randomforest_fulldata         0.000000  0.000000\n",
       "Randomforest_fulldata         0.000000  0.000000\n",
       "Randomforest_pca30            0.000000  0.000000\n",
       "Randomforest_pca30            0.000000  0.000000\n",
       "Randomforest_pca100           0.000000  0.000000\n",
       "Randomforest_pca100           0.000000  0.000000"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results, columns=columnas,index=filas)\n",
    "df_results.sort_values([\"RECALL\",\"PRECISION\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47d89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
